---
{
  "title": "Migrating from MultiPromptChain",
  "source_url": "https://python.langchain.com/docs/versions/migrating_chains/multi_prompt_chain/",
  "fetched_at": "2025-08-15T13:52:01.685645+00:00"
}
---

# Migrating from MultiPromptChain

from
operator
import
itemgetter
from
typing
import
Literal
from
langchain_core
.
output_parsers
import
StrOutputParser
from
langchain_core
.
prompts
import
ChatPromptTemplate
from
langchain_core
.
runnables
import
RunnableConfig
from
langchain_openai
import
ChatOpenAI
from
langgraph
.
graph
import
END
,
START
,
StateGraph
from
typing_extensions
import
TypedDict
llm
=
ChatOpenAI
(
model
=
"gpt-4o-mini"
)
prompt_1
=
ChatPromptTemplate
.
from_messages
(
[
(
"system"
,
"You are an expert on animals."
)
,
(
"human"
,
"{input}"
)
,
]
)
prompt_2
=
ChatPromptTemplate
.
from_messages
(
[
(
"system"
,
"You are an expert on vegetables."
)
,
(
"human"
,
"{input}"
)
,
]
)
chain_1
=
prompt_1
|
llm
|
StrOutputParser
(
)
chain_2
=
prompt_2
|
llm
|
StrOutputParser
(
)
route_system
=
"Route the user's query to either the animal or vegetable expert."
route_prompt
=
ChatPromptTemplate
.
from_messages
(
[
(
"system"
,
route_system
)
,
(
"human"
,
"{input}"
)
,
]
)
class
RouteQuery
(
TypedDict
)
:
"""Route query to destination expert."""
destination
:
Literal
[
"animal"
,
"vegetable"
]
route_chain
=
route_prompt
|
llm
.
with_structured_output
(
RouteQuery
)
class
State
(
TypedDict
)
:
query
:
str
destination
:
RouteQuery
answer
:
str
async
def
route_query
(
state
:
State
,
config
:
RunnableConfig
)
:
destination
=
await
route_chain
.
ainvoke
(
state
[
"query"
]
,
config
)
return
{
"destination"
:
destination
}
async
def
prompt_1
(
state
:
State
,
config
:
RunnableConfig
)
:
return
{
"answer"
:
await
chain_1
.
ainvoke
(
state
[
"query"
]
,
config
)
}
async
def
prompt_2
(
state
:
State
,
config
:
RunnableConfig
)
:
return
{
"answer"
:
await
chain_2
.
ainvoke
(
state
[
"query"
]
,
config
)
}
def
select_node
(
state
:
State
)
-
>
Literal
[
"prompt_1"
,
"prompt_2"
]
:
if
state
[
"destination"
]
==
"animal"
:
return
"prompt_1"
else
:
return
"prompt_2"
graph
=
StateGraph
(
State
)
graph
.
add_node
(
"route_query"
,
route_query
)
graph
.
add_node
(
"prompt_1"
,
prompt_1
)
graph
.
add_node
(
"prompt_2"
,
prompt_2
)
graph
.
add_edge
(
START
,
"route_query"
)
graph
.
add_conditional_edges
(
"route_query"
,
select_node
)
graph
.
add_edge
(
"prompt_1"
,
END
)
graph
.
add_edge
(
"prompt_2"
,
END
)
app
=
graph
.
compile
(
)
