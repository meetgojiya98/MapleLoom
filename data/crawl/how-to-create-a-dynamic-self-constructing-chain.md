---
{
  "title": "How to create a dynamic (self-constructing) chain",
  "source_url": "https://python.langchain.com/docs/how_to/dynamic_chain/",
  "fetched_at": "2025-08-15T13:50:01.423124+00:00"
}
---

# How to create a dynamic (self-constructing) chain

from
operator
import
itemgetter
from
langchain_core
.
output_parsers
import
StrOutputParser
from
langchain_core
.
prompts
import
ChatPromptTemplate
from
langchain_core
.
runnables
import
Runnable
,
RunnablePassthrough
,
chain
contextualize_instructions
=
"""Convert the latest user question into a standalone question given the chat history. Don't answer the question, return the question and nothing else (no descriptive text)."""
contextualize_prompt
=
ChatPromptTemplate
.
from_messages
(
[
(
"system"
,
contextualize_instructions
)
,
(
"placeholder"
,
"{chat_history}"
)
,
(
"human"
,
"{question}"
)
,
]
)
contextualize_question
=
contextualize_prompt
|
llm
|
StrOutputParser
(
)
qa_instructions
=
(
"""Answer the user question given the following context:\n\n{context}."""
)
qa_prompt
=
ChatPromptTemplate
.
from_messages
(
[
(
"system"
,
qa_instructions
)
,
(
"human"
,
"{question}"
)
]
)
@chain
def
contextualize_if_needed
(
input_
:
dict
)
-
>
Runnable
:
if
input_
.
get
(
"chat_history"
)
:
return
contextualize_question
else
:
return
RunnablePassthrough
(
)
|
itemgetter
(
"question"
)
@chain
def
fake_retriever
(
input_
:
dict
)
-
>
str
:
return
"egypt's population in 2024 is about 111 million"
full_chain
=
(
RunnablePassthrough
.
assign
(
question
=
contextualize_if_needed
)
.
assign
(
context
=
fake_retriever
)
|
qa_prompt
|
llm
|
StrOutputParser
(
)
)
full_chain
.
invoke
(
{
"question"
:
"what about egypt"
,
"chat_history"
:
[
(
"human"
,
"what's the population of indonesia"
)
,
(
"ai"
,
"about 276 million"
)
,
]
,
}
)
