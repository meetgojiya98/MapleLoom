---
{
  "title": "How to cache chat model responses",
  "source_url": "https://python.langchain.com/docs/how_to/chat_model_caching/",
  "fetched_at": "2025-08-15T13:49:38.453406+00:00"
}
---

# How to cache chat model responses

AIMessage(content="Why don't scientists trust atoms?\n\nBecause they make up everything!", response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 11, 'total_tokens': 24}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-b6836bdd-8c30-436b-828f-0ac5fc9ab50e-0')
